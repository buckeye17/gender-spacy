{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d272bbb-3e67-45c4-b401-ba065bbf4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "from typing import List, Union\n",
    "\n",
    "import requests\n",
    "import tqdm  # progress bar\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae03275d-9e2e-4487-8f1e-28ea524c6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"xlm_roberta\": {\n",
    "        \"url\": \"https://storage.googleapis.com/pandora-intelligence/models/crosslingual-coreference/xlm-roberta-base/model.tar.gz\",  # noqa: B950\n",
    "        \"f1_score_ontonotes\": 74,\n",
    "        \"file_extension\": \".tar.gz\",\n",
    "    },\n",
    "    \"info_xlm\": {\n",
    "        \"url\": \"https://storage.googleapis.com/pandora-intelligence/models/crosslingual-coreference/infoxlm-base/model.tar.gz\",  # noqa: B950\n",
    "        \"f1_score_ontonotes\": 77,\n",
    "        \"file_extension\": \".tar.gz\",\n",
    "    },\n",
    "    \"minilm\": {\n",
    "        \"url\": (\n",
    "            \"https://storage.googleapis.com/pandora-intelligence/models/crosslingual-coreference/minilm/model.tar.gz\"\n",
    "        ),\n",
    "        \"f1_score_ontonotes\": 74,\n",
    "        \"file_extension\": \".tar.gz\",\n",
    "    },\n",
    "    \"spanbert\": {\n",
    "        \"url\": \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\",\n",
    "        \"f1_score_ontonotes\": 83,\n",
    "        \"file_extension\": \".tar.gz\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb6974bd-863d-45df-9db7-d2b9c1c36b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\wma22\\AppData\\Local\\Temp\\tmpnekkyml0\\config.json as plain json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'top_spans': [[2, 2], [4, 5], [7, 7], [8, 8], [9, 10], [12, 12], [14, 14], [19, 19], [22, 22], [23, 26], [25, 25]], 'antecedent_indices': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], 'predicted_antecedents': [-1, -1, 1, -1, -1, -1, -1, 6, -1, -1, 2], 'document': ['Do', 'not', 'forget', 'about', 'Momofuku', 'Ando', '!', 'He', 'created', 'instant', 'noodles', 'in', 'Osaka', '.', 'I', 'do', \"n't\", 'know', 'if', 'I', 'like', 'to', 'eat', 'the', 'noodles', 'he', 'makes', '.'], 'clusters': [[[4, 5], [7, 7], [25, 25]], [[14, 14], [19, 19]]]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[4, 5], [7, 7], [25, 25]], [[14, 14], [19, 19]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossLingualPredictor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        language: str,\n",
    "        device: int = -1,\n",
    "        model_name: str = \"minilm\",\n",
    "        chunk_size: Union[int, None] = None,  # determines the # sentences per batch\n",
    "        chunk_overlap: int = 2,  # determines the # of overlapping sentences per chunk\n",
    "    ) -> None:\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.language = language\n",
    "        self.filename = None\n",
    "        self.device = device\n",
    "        self.model_url = MODELS[model_name][\"url\"]\n",
    "        # self.resolver = Resolver()\n",
    "        self.download_model()\n",
    "        self.set_coref_model()\n",
    "\n",
    "    def download_model(self):\n",
    "        \"\"\"\n",
    "        It downloads the model from the url provided and saves it in the current directory\n",
    "        \"\"\"\n",
    "        if \"https://storage.googleapis.com/pandora-intelligence/\" in self.model_url:\n",
    "            self.filename = self.model_url.replace(\"https://storage.googleapis.com/pandora-intelligence/\", \"\")\n",
    "        else:\n",
    "            self.filename = self.model_url.replace(\"https://storage.googleapis.com/allennlp-public-models/\", \"\")\n",
    "        path = pathlib.Path(self.filename)\n",
    "        if path.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            path.parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "            r = requests.get(self.model_url, stream=True)\n",
    "            file_size = int(r.headers[\"Content-Length\"])\n",
    "\n",
    "            chunk_size = 1024\n",
    "            num_bars = int(file_size / chunk_size)\n",
    "\n",
    "            with open(self.filename, \"wb\") as fp:\n",
    "                for chunk in tqdm.tqdm(\n",
    "                    r.iter_content(chunk_size=chunk_size),\n",
    "                    total=num_bars,\n",
    "                    unit=\"KB\",\n",
    "                    desc=self.filename,\n",
    "                    leave=True,\n",
    "                ):\n",
    "                    fp.write(chunk)\n",
    "\n",
    "    def set_coref_model(self):\n",
    "        \"\"\"Initialize AllenNLP coreference model\"\"\"\n",
    "        self.predictor = Predictor.from_path(self.filename, language=self.language, cuda_device=self.device)\n",
    "\n",
    "    def predict(self, text: str) -> dict:\n",
    "        \"\"\"predict and rsolve\n",
    "        Args:\n",
    "            text (str): an input text\n",
    "            uses more advanced resolve from:\n",
    "            https://towardsdatascience.com/how-to-make-an-effective-coreference-resolution-model-55875d2b5f19.\n",
    "        Returns:\n",
    "            dict: a prediciton\n",
    "        \"\"\"\n",
    "        # chunk text\n",
    "        doc = self.predictor._spacy(text)\n",
    "        if self.chunk_size:\n",
    "            chunks = self.chunk_sentencized_doc(doc)\n",
    "        else:\n",
    "            chunks = [text]\n",
    "\n",
    "        # make predictions for individual chunks\n",
    "        json_batch = [{\"document\": chunk} for chunk in chunks]\n",
    "        predictions = self.predictor.predict_batch_json(json_batch)\n",
    "        print(predictions)\n",
    "\n",
    "        # determine doc_lengths to resolve overlapping chunks\n",
    "        doc_lengths = [\n",
    "            sum([len(sent) for sent in list(doc_chunk.sents)[:-2]]) for doc_chunk in self.predictor._spacy.pipe(chunks)\n",
    "        ]\n",
    "        doc_lengths = [0] + doc_lengths[:-1]\n",
    "\n",
    "        # convert cluster predictions to their original index in doc\n",
    "        all_clusters = [pred[\"clusters\"] for pred in predictions]\n",
    "        corrected_clusters = []\n",
    "        for idx, doc_clus in enumerate(all_clusters):\n",
    "            corrected_clusters.append(\n",
    "                [[[num + sum(doc_lengths[: idx + 1]) for num in span] for span in clus] for clus in doc_clus]\n",
    "            )\n",
    "        merged_clusters = self.merge_clusters(corrected_clusters)\n",
    "        return merged_clusters\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_clusters(\n",
    "        clusters: List[List[List[int]]],\n",
    "    ) -> List[List[List[int]]]:\n",
    "        \"\"\"merge overlapping cluster from different segments, based on n_overlap_sentences\"\"\"\n",
    "        main_doc_clus = []\n",
    "        for doc_clus in clusters:\n",
    "            for clus in doc_clus:\n",
    "                combine_clus = False\n",
    "                for span in clus:\n",
    "                    for main_clus in main_doc_clus:\n",
    "                        for main_span in main_clus:\n",
    "                            if main_span == span:\n",
    "                                combined_clus = main_clus + clus\n",
    "                                combined_clus.sort()\n",
    "                                combined_clus = list(k for k, _ in itertools.groupby(combined_clus))\n",
    "                                combine_clus = True\n",
    "                                break\n",
    "                        if combine_clus:\n",
    "                            break\n",
    "                    if combine_clus:\n",
    "                        break\n",
    "                if combine_clus:\n",
    "                    main_doc_clus.append(combined_clus)\n",
    "                else:\n",
    "                    main_doc_clus.append(clus)\n",
    "\n",
    "        main_doc_clus.sort()\n",
    "        main_doc_clus = list(k for k, _ in itertools.groupby(main_doc_clus))\n",
    "        return main_doc_clus\n",
    "nlp = CrossLingualPredictor(\"en_core_web_sm\")\n",
    "nlp.predict(\"Do not forget about Momofuku Ando! He created instant noodles in Osaka. I don't know if I like to eat the noodles he makes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee5108-b407-4f9c-b8d5-8f0d337f9929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
